# -*- coding: utf-8 -*-
"""torch_basics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P7RoRKwNvsPKK9oq5Y8dGJMr7Sh7I5kg

# PyTorch basics

### package
"""

import os
import torch
import numpy as np

torch.cuda.is_available()
!nvidia-smi

"""## Tensor

### create tensor directly
"""

# create tensor
# torch.tensor(data, dtype=None, device=None, requires_grad=False, pin_memory=False)
arr = np.ones((3,3))
print(arr.dtype)
print(arr)
t1 = torch.tensor(arr)
print(t1)

t2 = torch.from_numpy(arr)
print(t2)

arr[0,0] = 2
print(t1)
print(t2)

# create tensor with 0s
# torch.zeros(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)
out_z=torch.tensor([1])
z=torch.zeros((3,3),out=out_z)
print(id(z),id(out_z), id(z)==id(out_z))
print(torch.ones(3,3))

# create 0 tensor by scale
# torch.zeros_like(input, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format)
ipt=torch.tensor([[1,2,3,4],[2,3,4,5],[3,4,5,6]])
torch.zeros_like(ipt)

# torch.full(size, fill_value, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)
size = (3,3,3)
full_value=6
f = torch.tensor(size)
print(torch.full(size,full_value))
print(torch.full_like(f,full_value))

# a range of numbers with equal distance
# torch.arange(start=0, end, step=1, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)
print(torch.arange(2,10))
print(torch.arange(2,10,2))

# uniformly divided
# torch.linspace(start, end, steps=100, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)
print(torch.linspace(2,10,5))
print(torch.linspace(2,20,5))

# torch.logspace(start, end, steps=100, base=10.0, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)
torch.logspace(2,10,6)

# create diagonal matrix
# torch.eye(n, m=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)
print(torch.eye(4))
print(torch.eye(3,4))

"""### create tensor in probability"""

# normal distribution
# torch.normal(mean, std, *, generator=None, out=None)
size=(3,4)
torch.normal(0,1,size)

# standard normal distribution
# torch.randn(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)
size=(3,4)
torch.randn(size)

# uniform distribution range(0,1)
# torch.rand(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)
torch.rand((3,4))

# random integer in a given range
# randint(low=0, high, size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)
size=(3,4)
torch.randint(0,11,size)

# integers random permutation
# torch.randperm(n, out=None, dtype=torch.int64, layout=torch.strided, device=None, requires_grad=False)
torch.randperm(3)

# bernouli distribution
# torch.bernoulli(input, *, generator=None, out=None)
torch.bernoulli(torch.tensor([0.3, 0.6, 0.8]))

"""## Operation"""

# concatenate tensors
# torch.cat(tensors, dim=0, out=None)
a = torch.ones((3,3))
b = torch.zeros((3,3))
c_0 = torch.cat([a,b],dim=0)
c_1 = torch.cat([a,b],dim=1)
print(c_0)
print(c_1)

# stack
# torch.stack(tensors, dim=0, out=None)
t=torch.ones(2,3)
print(t)
t_stack=torch.stack([t,t,t],dim=2)
print(t_stack)
t_stack=torch.stack([t,t,t],dim=0)
print(t_stack)

# chunk
# torch.chunk(input, chunks, dim=0)
a = torch.ones((2,7))
print(a)
list_of_tensors=torch.chunk(a,dim=1,chunks=3)
for i, t in enumerate(list_of_tensors):
  print("No.{} tensor:{}, shape is {}".format(i+1,t,t.shape))

# split
# torch.split(tensor, split_size_or_sections, dim=0)
t = torch.ones((2,5))
list_of_tensors = torch.split(t,[2,1,2], dim=1)
for i, t in enumerate(list_of_tensors):
  print("No.{} tensor: {}, shape is {}".format(i,t,t.shape))

# select index
# torch.index_select(input, dim, index, out=None)
t = torch.randint(0,9,size=(3,3))
i = torch.tensor([0,2],dtype=torch.long)
t_select = torch.index_select(t,dim=0,index=i)
print("t:\n{}\nt_select:\n{}".format(t,t_select))

# select mask
# torch.masked_select(input, mask, out=None)
t = torch.randint(0,9,size=(3,3))
mask = t.le(5)
t_select = torch.masked_select(t,mask)
print("t:\n{}\nmask:\n{}\nt_select:\n{} ".format(t, mask, t_select))

# reshape
# torch.reshape(input, shape)
t = torch.randperm(8)
t_reshape = torch.reshape(t,(-1,2,2))
print(t)
print(t_reshape)

# transpose
# torch.transpose(input, dim0, dim1)
t = torch.rand(2,3,4)
t_transpose=torch.transpose(t,dim0=1,dim1=2)
print(t)
print(t_transpose)

# squeeze, compress the dimension with length=1
# torch.squeeze(input, dim=None, out=None)
t = torch.rand((1,2,3,1))
t_sq = torch.squeeze(t)
t_0 = torch.squeeze(t,dim=0)
t_1 = torch.squeeze(t,dim=1)
print(t)
print(t_sq)
print(t_0)
print(t_1)

# unsqueeze, extend the dimension
# torch.unsqueeze(input, dim)

"""## Mathematical operation"""

# add
# torch.add(input, other, out=None)
# torch.add(input, other, *, alpha=1, out=None)
# torch.addcdiv(input, tensor1, tensor2, *, value=1, out=None)
# torch.addcmul(input, tensor1, tensor2, *, value=1, out=None)
t_0=torch.tensor([1,2,3])
t_1=torch.tensor([2,3,4])
torch.add(t_0,t_1)

"""single-value linear regression
$$
y=wx+b
$$
"""

import torch
import matplotlib.pyplot as plt
torch.manual_seed(10)

lr = 0.05

# create training data
x = torch.rand(20, 1) * 10  # x data (tensor), shape=(20, 1)
# noise
y = 2*x + (5 + torch.randn(20, 1))  # y data (tensor), shape=(20, 1)

# parameter
w = torch.randn((1), requires_grad=True) 
b = torch.zeros((1), requires_grad=True) 

# training process
for epoch in range(1001):

    # compute prediction y
    wx = torch.mul(w, x)
    y_pred = torch.add(wx, b)

    # compute loss MSE
    loss = (0.5 * (y - y_pred) ** 2).mean()

    # backpropagation
    loss.backward()

    # update parameter
    b.data.sub_(lr * b.grad)
    w.data.sub_(lr * w.grad)

    # clear the gradient
    w.grad.zero_()
    b.grad.zero_()

    # plot
    if epoch % 100 == 0:

        print("epoch:{} train loss:{}".format(epoch,loss))

plt.scatter(x.data.numpy(), y.data.numpy())
plt.plot(x.data.numpy(), y_pred.data.numpy(), 'r-', lw=5)
plt.text(2, 20, 'Loss=%.4f' % loss.data.numpy(), fontdict={'size': 20, 'color':  'red'})
plt.xlim(1.5, 10)
plt.ylim(8, 28)
plt.title("epoch: {}\nw: {} b: {}".format(epoch, w.data.numpy(), b.data.numpy()))

